{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjpena35226/q-learning/blob/main/car/ai_race_game_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgOvFD4HGXwM",
        "outputId": "feaa4511-6897-43b7-cc0d-1354935f3a52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-GdWLBnhKkT",
        "outputId": "2d0a0396-0be2-4149-f66b-8f21cf6b447d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.1.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.8 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: swig, pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0 swig-4.1.1.post0\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium)\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.1.1.post0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373076 sha256=72b77a37323d1bdc12de9d5cd8350dda475f4733f6b792e61d37122d71682857\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: farama-notifications, box2d-py, gymnasium\n",
            "Successfully installed box2d-py-2.3.5 farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "# HIDE OUTPUT\n",
        "!pip install swig pyvirtualdisplay\n",
        "!pip install gymnasium gymnasium[box2d] tqdm\n",
        "#!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oneVcQoQskC8"
      },
      "outputs": [],
      "source": [
        "# HIDE OUTPUT\n",
        "#!apt-get update > /dev/null 2>&1\n",
        "#!apt-get install cmake > /dev/null 2>&1\n",
        "#!apt-get install xvfb > /dev/null 2>&1\n",
        "#!pip install ez_setup > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Hy6gLawG7DZ",
        "outputId": "e7d986fd-f707-4075-9164-01a5674045f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.14.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.executing_eagerly()\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PmfdZhyMPOpL"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gym.wrappers import RecordVideo\n",
        "from pyvirtualdisplay import Display\n",
        "from gymnasium.utils.save_video import save_video\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import HTML, clear_output\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from pathlib import Path\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import cv2\n",
        "import argparse\n",
        "from collections import deque\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PARAMETERS"
      ],
      "metadata": {
        "id": "nCKW5DXq8oGd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EVxRZfgbMCYF"
      },
      "outputs": [],
      "source": [
        "RENDER                        = True\n",
        "STARTING_EPISODE              = 1\n",
        "ENDING_EPISODE                = 200\n",
        "SKIP_FRAMES                   = 2\n",
        "TRAINING_BATCH_SIZE           = 64\n",
        "SAVE_TRAINING_FREQUENCY       = 25\n",
        "UPDATE_TARGET_MODEL_FREQUENCY = 5\n",
        "EPSILON                       = 0.1\n",
        "MODEL_NAME                    = 'car-model'\n",
        "MODEL_FILE                    = MODEL_NAME + '_25.h5'\n",
        "ENV_URL = '/content/drive/MyDrive/q-learning/car'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1beN2D1YKBax"
      },
      "source": [
        "# UTILS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY5UpNp6HkP1",
        "outputId": "b20eec25-3eab-4dc4-f4e8-99abf120bc09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(gpus)\n",
        "gpuName = 'device:GPU:0' if gpus != None else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9p0BZo4FsIwX"
      },
      "outputs": [],
      "source": [
        "def save_video_to(env, episode = 0, video_folder = \"/video\"):\n",
        "    save_video(\n",
        "         env.render(),\n",
        "         video_folder=ENV_URL + video_folder,\n",
        "         fps=env.metadata[\"render_fps\"],\n",
        "         name_prefix=episode\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def render_env(env):\n",
        "  clear_output(wait=True)\n",
        "  plt.imshow(env.render())\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "hxgai-Dn3muu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yIv6kkjlKVYc"
      },
      "outputs": [],
      "source": [
        "def process_state_image(state):\n",
        "    state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)\n",
        "    state = state.astype(float)\n",
        "    state /= 255.0\n",
        "    return np.reshape(state, [96,96,1])\n",
        "\n",
        "def generate_state_frame_stack_from_queue(deque):\n",
        "    frame_stack = np.array(deque)\n",
        "    # Move stack dimension to the channel dimension (stack, x, y) -> (x, y, stack)\n",
        "    return np.transpose(frame_stack, (1, 2, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eW3BiwFJ1XJ"
      },
      "source": [
        "# DEEP-Q-LEARNING-CAR-AGENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tGnX9E194QoS"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "class CarRacingDQNAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_shape,\n",
        "        action_space    = [\n",
        "            (-1, 1, 0.2), (0, 1, 0.2), (1, 1, 0.2), #           Action Space Structure\n",
        "            (-1, 1,   0), (0, 1,   0), (1, 1,   0), #        (Steering Wheel, Gas, Break)\n",
        "            (-1, 0, 0.2), (0, 0, 0.2), (1, 0, 0.2), # Range        -1~1       0~1   0~1\n",
        "            (-1, 0,   0), (0, 0,   0), (1, 0,   0)\n",
        "        ],\n",
        "        frame_stack_num = 3,\n",
        "        memory_size     = 5000,\n",
        "        gamma           = 0.95,  # discount rate\n",
        "        epsilon         = 1.0,   # exploration rate\n",
        "        epsilon_min     = 0.1,\n",
        "        epsilon_decay   = 0.9999,\n",
        "        learning_rate   = 0.001,\n",
        "    ):\n",
        "        self.observation_shape=observation_shape\n",
        "        self.action_space    = action_space\n",
        "        self.frame_stack_num = frame_stack_num\n",
        "        self.memory          = []\n",
        "        self.memory_size     = memory_size\n",
        "        self.gamma           = gamma\n",
        "        self.epsilon         = epsilon\n",
        "        self.epsilon_min     = epsilon_min\n",
        "        self.epsilon_decay   = epsilon_decay\n",
        "        self.learning_rate   = learning_rate\n",
        "        self.model           = self.build_model()\n",
        "        self.target_model    = self.build_model()\n",
        "        self.update_target_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(Conv2D(filters=6, kernel_size=(7, 7), activation='relu', input_shape=self.observation_shape))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Conv2D(filters=12, kernel_size=(4, 4), activation='relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(216, activation='relu'))\n",
        "        model.add(Dense(len(self.action_space), activation=None))\n",
        "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=self.learning_rate, epsilon=1e-7))\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def clearMemory(self):\n",
        "        del self.memory\n",
        "        self.memory = []\n",
        "\n",
        "    def memorize(self, state, action, reward, next_state, done):\n",
        "        if(len(self.memory) >= self.memory_size - 1):\n",
        "          removed = [self.memory.pop(random.randrange(len(self.memory))) for _ in range(1)]\n",
        "        self.memory.append((state, self.action_space.index(action), reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() > self.epsilon:\n",
        "            act_values = self.model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
        "            action_index = np.argmax(act_values[0])\n",
        "        else:\n",
        "            action_index = random.randrange(len(self.action_space))\n",
        "        return self.action_space[action_index]\n",
        "\n",
        "    def internalReplay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        states = np.array([exp[0] for exp in minibatch])\n",
        "        next_states = np.array([exp[3] for exp in minibatch])\n",
        "\n",
        "        targets = self.model.predict(states, verbose = 0)\n",
        "        next_targets = self.target_model.predict(next_states, verbose = 0)\n",
        "\n",
        "        for index, experience in enumerate(minibatch):\n",
        "            state, action_index, reward, next_state, done = experience\n",
        "            if done:\n",
        "                targets[index][action_index] = reward\n",
        "            else:\n",
        "                \"\"\"Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\"\"\"\n",
        "                delta = (\n",
        "                      reward\n",
        "                      + self.gamma * np.amax(next_targets[index])\n",
        "                      - targets[index][action_index]\n",
        "                  )\n",
        "                targets[index][action_index] = targets[index][action_index] + self.learning_rate * delta\n",
        "\n",
        "        self.model.fit(states, np.array(targets), epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        if gpus:\n",
        "          for gpu in gpus:\n",
        "            with tf.device(gpuName):\n",
        "              self.internalReplay(batch_size)\n",
        "        else:\n",
        "            self.internalReplay(batch_size)\n",
        "\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "        self.model.summary()\n",
        "        self.update_target_model()\n",
        "\n",
        "    def save(self, name):\n",
        "        self.target_model.save_weights(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FvRBu0RKh71"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nMWeGP7Fi0kA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a35125-6ac8-4a0e-d093-cd5eccda17d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_12 (Conv2D)          (None, 90, 90, 6)         888       \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPooli  (None, 45, 45, 6)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 42, 42, 12)        1164      \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPooli  (None, 21, 21, 12)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 5292)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 216)               1143288   \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 12)                2604      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1147944 (4.38 MB)\n",
            "Trainable params: 1147944 (4.38 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CarRacing-v2\", domain_randomize=True, render_mode=\"rgb_array_list\")\n",
        "init_state, info = env.reset()\n",
        "#init_state = process_state_image(init_state)\n",
        "agent = CarRacingDQNAgent(observation_shape=init_state.shape, epsilon=EPSILON, memory_size=1000)\n",
        "if MODEL_FILE:\n",
        "        agent.load(ENV_URL + '/save/' + MODEL_FILE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--DKSUU4khIU",
        "outputId": "856c9080-af31-4946-f213-920d0905c58f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 11/200 [18:39<6:37:25, 126.17s/it]"
          ]
        }
      ],
      "source": [
        "for e in tqdm(range(ENDING_EPISODE), leave=False):\n",
        "        init_state, info = env.reset()\n",
        "\n",
        "        total_reward = 0\n",
        "        negative_reward_counter = 0\n",
        "        current_state = init_state #process_state_image(init_state)\n",
        "        time_frame_counter = 1\n",
        "        done = False\n",
        "        #agent.clearMemory()\n",
        "\n",
        "        while True:\n",
        "\n",
        "            action = agent.act(current_state)\n",
        "\n",
        "            reward = 0\n",
        "            for _ in range(SKIP_FRAMES+1):\n",
        "                next_state, r, done, truncated, info = env.step(action)\n",
        "                reward += r\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            # If continually getting negative reward 10 times after the tolerance steps, terminate this episode\n",
        "            negative_reward_counter = negative_reward_counter + 1 if time_frame_counter > 100 and reward < 0 else 0\n",
        "\n",
        "            # Extra bonus for the model if it uses full gas\n",
        "            if action[1] == 1 and action[2] == 0:\n",
        "                reward *= 1.5\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            agent.memorize(current_state, action, reward, next_state, done)\n",
        "\n",
        "            if done or negative_reward_counter >= 25 or total_reward < 0:\n",
        "                #print('Episode: {}/{}, Scores(Time Frames): {}, Total Rewards(adjusted): {:.2}, Epsilon: {:.2}'.format(e, ENDING_EPISODE, time_frame_counter, float(total_reward), float(agent.epsilon)))\n",
        "                break\n",
        "            if len(agent.memory) > TRAINING_BATCH_SIZE:\n",
        "                agent.replay(TRAINING_BATCH_SIZE)\n",
        "\n",
        "            time_frame_counter += 1\n",
        "            current_state = next_state\n",
        "\n",
        "        if e % UPDATE_TARGET_MODEL_FREQUENCY == 0:\n",
        "            agent.update_target_model()\n",
        "\n",
        "        if e % SAVE_TRAINING_FREQUENCY == 0:\n",
        "            agent.save(ENV_URL + '/save/' + MODEL_NAME +'_{}.h5'.format(e))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQBZvZy1Ns8f"
      },
      "source": [
        "# \"AUTONOMOUS\" DRIVING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mCe3eKeCNwn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1528b2-52dd-4001-c7ea-acb3f7807a49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [01:26<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/drive/MyDrive/q-learning/car/video/0-episode-0.mp4.\n",
            "Moviepy - Writing video /content/drive/MyDrive/q-learning/car/video/0-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "t:   0%|          | 0/1001 [00:00<?, ?it/s, now=None]\u001b[A\n",
            "t:   4%|▍         | 39/1001 [00:00<00:02, 384.86it/s, now=None]\u001b[A\n",
            "t:   8%|▊         | 78/1001 [00:00<00:03, 266.22it/s, now=None]\u001b[A\n",
            "t:  11%|█▏        | 113/1001 [00:00<00:03, 295.89it/s, now=None]\u001b[A\n",
            "t:  15%|█▌        | 152/1001 [00:00<00:02, 325.06it/s, now=None]\u001b[A\n",
            "t:  19%|█▉        | 188/1001 [00:00<00:02, 336.34it/s, now=None]\u001b[A\n",
            "t:  23%|██▎       | 227/1001 [00:00<00:02, 352.69it/s, now=None]\u001b[A\n",
            "t:  26%|██▋       | 264/1001 [00:00<00:02, 356.60it/s, now=None]\u001b[A\n",
            "t:  30%|███       | 301/1001 [00:00<00:01, 351.15it/s, now=None]\u001b[A\n",
            "t:  34%|███▍      | 338/1001 [00:00<00:01, 356.62it/s, now=None]\u001b[A\n",
            "t:  38%|███▊      | 376/1001 [00:01<00:01, 360.73it/s, now=None]\u001b[A\n",
            "t:  41%|████▏     | 413/1001 [00:01<00:01, 345.26it/s, now=None]\u001b[A\n",
            "t:  45%|████▍     | 448/1001 [00:01<00:01, 334.51it/s, now=None]\u001b[A\n",
            "t:  48%|████▊     | 482/1001 [00:01<00:01, 322.88it/s, now=None]\u001b[A\n",
            "t:  51%|█████▏    | 515/1001 [00:01<00:01, 315.10it/s, now=None]\u001b[A\n",
            "t:  55%|█████▍    | 547/1001 [00:01<00:01, 301.98it/s, now=None]\u001b[A\n",
            "t:  58%|█████▊    | 582/1001 [00:01<00:01, 312.75it/s, now=None]\u001b[A\n",
            "t:  61%|██████▏   | 614/1001 [00:01<00:01, 313.90it/s, now=None]\u001b[A\n",
            "t:  65%|██████▍   | 648/1001 [00:01<00:01, 318.91it/s, now=None]\u001b[A\n",
            "t:  68%|██████▊   | 682/1001 [00:02<00:00, 324.64it/s, now=None]\u001b[A\n",
            "t:  72%|███████▏  | 722/1001 [00:02<00:00, 344.04it/s, now=None]\u001b[A\n",
            "t:  76%|███████▌  | 761/1001 [00:02<00:00, 356.51it/s, now=None]\u001b[A\n",
            "t:  80%|███████▉  | 799/1001 [00:02<00:00, 354.93it/s, now=None]\u001b[A\n",
            "t:  84%|████████▎ | 837/1001 [00:02<00:00, 361.77it/s, now=None]\u001b[A\n",
            "t:  87%|████████▋ | 874/1001 [00:02<00:00, 358.87it/s, now=None]\u001b[A\n",
            "t:  91%|█████████ | 912/1001 [00:02<00:00, 364.79it/s, now=None]\u001b[A\n",
            "t:  95%|█████████▍| 949/1001 [00:02<00:00, 358.50it/s, now=None]\u001b[A\n",
            "t:  98%|█████████▊| 985/1001 [00:02<00:00, 358.66it/s, now=None]\u001b[A\n",
            "                                             "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/drive/MyDrive/q-learning/car/video/0-episode-0.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "#Reset environment\n",
        "init_state, info = env.reset()\n",
        "print('Driving...')\n",
        "\n",
        "total_reward = 0\n",
        "punishment_counter = 0\n",
        "current_state = init_state\n",
        "time_frame_counter = 1\n",
        "terminated = False\n",
        "\n",
        "while not terminated:\n",
        "  action = agent.act(current_state)\n",
        "  next_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "  terminated = done or truncated\n",
        "\n",
        "  total_reward += reward\n",
        "\n",
        "  current_state = next_state\n",
        "\n",
        "  if done:\n",
        "    #print('Episode: {}/{}, Scores(Time Frames): {}, Total Rewards: {:.2}'.format(e+1, time_frame_counter, float(total_reward)))\n",
        "    break\n",
        "  time_frame_counter += 1\n",
        "\n",
        "save_video_to(env=env, episode = e, video_folder = \"/video\")\n",
        "\n",
        "env.reset()\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwNjmfE6jgu0"
      },
      "outputs": [],
      "source": [
        "print('Original code in https://github.com/andywu0913/OpenAI-GYM-CarRacing-DQN')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "1beN2D1YKBax"
      ],
      "authorship_tag": "ABX9TyPAbjGxSHqC3y778XQ5/+PW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}